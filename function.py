import prompts
import logging
import time as time_module
import pandas as pd 
import os
import psycopg2
from datetime import datetime, timedelta, date,time
from decimal import Decimal
import json


def generate_sql_with_openai(prompt,client):
    try:
        sql_prompt=prompts.sql_prompt
        logging.info('started sql generation')
        response = client.chat.completions.create(
            model="gpt-4o-mini-2024-07-18",
            messages=[
                {
                    "role":"system",
                    "content":sql_prompt
                },
                {
                "role":"user",
                "content":prompt
                }
            ]
        )
        sql_query = response.choices[0].message.content
        logging.info(f"Generated SQL query: {sql_query}")
        return sql_query
        
    except Exception as e:
        logging.info(f"OpenAI API error: {str(e)}")
        raise



def generate_streaming_response(prompt_analysis, client2, prompts, wpm=350):
    """
    Generate streaming response for analysis
    """
    try:
        summary_prompt = prompts.summary_prompt
        logging.info('Started summary generation')
        
        response = client2.chat.completions.create(
            model="deepseek/deepseek-chat-v3.1:free",
            stream=True,
            messages=[
                {
                    "role": "system",
                    "content": summary_prompt
                },
                {
                    "role": "user",
                    "content": prompt_analysis
                },
            ]
        ) 
        
        for chunk in response:
            delta = chunk.choices[0].delta  
            logging.info(f'delta: {delta.__dict__}')
            
            if hasattr(delta, 'content') and delta.content:
                try:
                    yield delta.content.encode('utf-8')
                    char_count = len(delta.content)
                    delay = (char_count * 30.0) / (wpm * 5)
                    time_module.sleep(delay)
                except Exception as e:
                    logging.error(f'Streaming error: {str(e)}')
                    
    except Exception as e:
        logging.error(f'Response creation failed: {str(e)}')
        yield f"Error: {str(e)}".encode('utf-8')


#saving user message to file

def append_messages_to_excel(messages, filename='data/user_messages.xlsx'):
    if os.path.exists(filename):
        existing_df = pd.read_excel(filename)
        new_df = pd.DataFrame({'message': messages})
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = pd.DataFrame({'message': messages})
    combined_df.to_excel(filename, index=False)

#saving summary generated by llm to file

def append_summaries_to_excel(summary, filename='data/summary.xlsx'):
    if os.path.exists(filename):
        existing_df = pd.read_excel(filename)
        new_df = pd.DataFrame({'summary': summary})
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = pd.DataFrame({'summary': summary})
    combined_df.to_excel(filename, index=False)

#saving the sql generated

def append_sql_to_excel(sql, filename='data/sql.xlsx'):
    if os.path.exists(filename):
        existing_df = pd.read_excel(filename)
        new_df = pd.DataFrame({'sql': sql})
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
    else:
        combined_df = pd.DataFrame({'sql': sql})
    combined_df.to_excel(filename, index=False)




class SafeJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder for PostgreSQL data types"""
    def default(self, obj):
        if isinstance(obj, (datetime, date, time)):
            return obj.isoformat()
        elif isinstance(obj, timedelta):
            return str(obj.total_seconds())  # Convert to seconds
        elif isinstance(obj, Decimal):
            return float(obj)
        return super().default(obj)


def execute_query_and_get_json(db_url, sql_query):

    try:
        with psycopg2.connect(db_url) as conn:
            with conn.cursor() as cur:
                cur.execute(sql_query)
                rows = cur.fetchall()
                colnames = [desc[0] for desc in cur.description]
                
                # Convert rows to serializable format
                list_of_dicts = []
                for row in rows:
                    row_dict = {}
                    for col, val in zip(colnames, row):
                        if isinstance(val, (datetime, date)):
                            row_dict[col] = val.isoformat()
                        elif isinstance(val, timedelta):
                            row_dict[col] = val.total_seconds()
                        elif isinstance(val, Decimal):
                            row_dict[col] = float(val)
                        else:
                            row_dict[col] = val
                    list_of_dicts.append(row_dict)
                
                try:
                    db_data_json = json.dumps(list_of_dicts, indent=2, cls=SafeJSONEncoder)
                    logging.info(f'Database fetching successful: {str(db_data_json)}')
                    return db_data_json, True
                except Exception as json_error:
                    logging.error(f'JSON serialization error: {str(json_error)}')
                    return None, False
                    
    except psycopg2.Error as db_error:
        logging.error(f'Database error: {str(db_error)}')
        return None, False
    except Exception as general_error:
        logging.error(f'General error in query execution: {str(general_error)}')
        return None, False
    

def generate_openai_reply(prompt,client):
    try:
        response = client.chat.completions.create(
            model="gpt-4",  # or your preferred model
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are Leajlak's customer service assistant (Leajlak's Order Management System connects merchants "
                        "and third-party logistics companies for on-demand express and scheduled deliveries, leveraging AI, IoT, "
                        "and Big Data to boost efficiency, cut costs, and improve customer satisfaction). "
                        "Check the user's message and send appropriate replies that are always inside the above context."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=150,
            temperature=0.7,
        )
        reply = response.choices[0].message.content.strip()
        return (str(reply))
    except Exception as e:
        logging.error(f"OpenAI API error: {e}")
        return "Sorry, something went wrong while generating the reply."